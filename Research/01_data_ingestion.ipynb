{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4b0a4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1+cpu\n",
      "CUDA available: False\n",
      "CUDA version: None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ae977f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Projects\\\\Quran-App-Backend\\\\Research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4699b2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "066530cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Projects\\\\Quran-App-Backend'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4cabfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Model\n",
    "\n",
    "# -----------------------------\n",
    "# Model Setup\n",
    "# -----------------------------\n",
    "MODEL_NAME = \"facebook/wav2vec2-large-xlsr-53\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(MODEL_NAME)\n",
    "model = Wav2Vec2Model.from_pretrained(MODEL_NAME).to(device)\n",
    "\n",
    "# -----------------------------\n",
    "# Extract Embedding Function\n",
    "# -----------------------------\n",
    "def extract_embedding(file_path):\n",
    "    y, sr = librosa.load(file_path, sr=16000)\n",
    "    inputs = feature_extractor(y, sampling_rate=sr, return_tensors=\"pt\", padding=True)\n",
    "    input_values = inputs.input_values.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_values)\n",
    "        embedding = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "\n",
    "    return embedding\n",
    "\n",
    "# -----------------------------\n",
    "# Compare Two Audio Files\n",
    "# -----------------------------\n",
    "def similarity_wav2vec(file1, file2):\n",
    "    emb1 = extract_embedding(file1)\n",
    "    emb2 = extract_embedding(file2)\n",
    "    score = cosine_similarity(emb1, emb2)[0][0]\n",
    "    return round(score * 100, 2)\n",
    "\n",
    "# -----------------------------\n",
    "# Match User Input Against All Reference Folders\n",
    "# -----------------------------\n",
    "def match_alphabet(user_audio, reference_root):\n",
    "    results = []\n",
    "\n",
    "    # Each subfolder = one alphabet\n",
    "    for folder_name in os.listdir(reference_root):\n",
    "        folder_path = os.path.join(reference_root, folder_name)\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "\n",
    "        best_score_for_letter = 0.0\n",
    "\n",
    "        # Compare user audio with every .wav file in this alphabet folder\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            if file_name.endswith(\".wav\"):\n",
    "                ref_path = os.path.join(folder_path, file_name)\n",
    "                try:\n",
    "                    score = similarity_wav2vec(ref_path, user_audio)\n",
    "                    best_score_for_letter = max(best_score_for_letter, score)\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è Skipping {ref_path}: {e}\")\n",
    "\n",
    "        results.append((folder_name, best_score_for_letter))\n",
    "\n",
    "    # Sort results by best similarity\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    best_match = results[0] if results else (\"None\", 0.0)\n",
    "    return best_match, results\n",
    "\n",
    "# -----------------------------\n",
    "# Example Usage\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    reference_root = \"Data/Arabic_Alphabets\"   # folder with subfolders for each alphabet\n",
    "    user_input = \"Data/user_input1.wav\"         # user spoken file\n",
    "\n",
    "    best_match, all_results = match_alphabet(user_input, reference_root)\n",
    "\n",
    "    print(f\"\\n‚úÖ Best Match: {best_match[0]} with {best_match[1]}% similarity\\n\")\n",
    "\n",
    "    print(\"üìä Full Ranking:\")\n",
    "    for letter, score in all_results:\n",
    "        print(f\"{letter:10s} ‚Üí {score}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f3c182",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Model\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "MODEL_NAME = \"facebook/wav2vec2-large-xlsr-53\"\n",
    "REFERENCE_ROOT = \"Data/Arabic_Alphabets\"   # Path to all alphabet folders\n",
    "TARGET_LETTER = \"ÿ®ÿßÿ°\"  //                    # <-- change this to the letter you want to evaluate\n",
    "MODEL_FILE = f\"{TARGET_LETTER}_model.pkl\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ============================================================\n",
    "# MODEL LOADING\n",
    "# ============================================================\n",
    "\n",
    "print(\"üîπ Loading Wav2Vec2 model...\")\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(MODEL_NAME)\n",
    "model = Wav2Vec2Model.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "print(\"‚úÖ Model loaded successfully.\\n\")\n",
    "\n",
    "# ============================================================\n",
    "# FEATURE EXTRACTION\n",
    "# ============================================================\n",
    "\n",
    "def extract_embedding(file_path):\n",
    "    \"\"\"Extract mean Wav2Vec2 embedding for a single .wav file.\"\"\"\n",
    "    y, sr = librosa.load(file_path, sr=16000)\n",
    "    inputs = feature_extractor(y, sampling_rate=sr, return_tensors=\"pt\", padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs.input_values.to(DEVICE))\n",
    "        return outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "\n",
    "# ============================================================\n",
    "# BUILD LETTER MODEL\n",
    "# ============================================================\n",
    "\n",
    "def build_letter_model(letter_folder, save_as):\n",
    "    \"\"\"Compute average embedding from all .wav files of a specific letter.\"\"\"\n",
    "    print(f\"üéôÔ∏è Building model for '{TARGET_LETTER}' from {letter_folder} ...\")\n",
    "    embeddings = []\n",
    "    for file in os.listdir(letter_folder):\n",
    "        if file.endswith(\".wav\"):\n",
    "            path = os.path.join(letter_folder, file)\n",
    "            emb = extract_embedding(path)\n",
    "            embeddings.append(emb)\n",
    "            print(f\"  ‚úÖ Processed: {file}\")\n",
    "    if not embeddings:\n",
    "        raise ValueError(\"No .wav files found in the given folder.\")\n",
    "    model_emb = np.mean(np.vstack(embeddings), axis=0, keepdims=True)\n",
    "    with open(save_as, \"wb\") as f:\n",
    "        pickle.dump(model_emb, f)\n",
    "    print(f\"‚úÖ Model saved as {save_as}\\n\")\n",
    "    return model_emb\n",
    "\n",
    "# ============================================================\n",
    "# COMPARE USER AUDIO\n",
    "# ============================================================\n",
    "\n",
    "def check_pronunciation(user_audio, model_file):\n",
    "    \"\"\"Compare user pronunciation with reference model.\"\"\"\n",
    "    with open(model_file, \"rb\") as f:\n",
    "        ref_emb = pickle.load(f)\n",
    "    user_emb = extract_embedding(user_audio)\n",
    "    score = cosine_similarity(ref_emb, user_emb)[0][0]\n",
    "    similarity = round(score * 100, 2)\n",
    "    print(f\"üéß Similarity to '{TARGET_LETTER}': {similarity}%\")\n",
    "    return similarity\n",
    "\n",
    "# ============================================================\n",
    "# MAIN SCRIPT\n",
    "# ============================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    letter_folder = os.path.join(REFERENCE_ROOT, TARGET_LETTER)\n",
    "\n",
    "    # 1Ô∏è‚É£ Build the model if not already saved\n",
    "    if not os.path.exists(MODEL_FILE):\n",
    "        build_letter_model(letter_folder, MODEL_FILE)\n",
    "\n",
    "    # 2Ô∏è‚É£ Compare user pronunciation\n",
    "    user_audio = \"Data/user_input.wav\"  # or change to user_input1.wav etc.\n",
    "    similarity = check_pronunciation(user_audio, MODEL_FILE)\n",
    "\n",
    "    # 3Ô∏è‚É£ Interpretation\n",
    "    if similarity >= 90:\n",
    "        print(\"‚úÖ Excellent pronunciation! (Correct)\")\n",
    "    elif 70 <= similarity < 90:\n",
    "        print(\"‚ö†Ô∏è Close! Try improving pronunciation.\")\n",
    "    else:\n",
    "        print(\"‚ùå Incorrect pronunciation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d951531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import librosa\n",
    "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Model\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "MODEL_NAME = \"facebook/wav2vec2-large-xlsr-53\"  # or smaller variant if you want speed\n",
    "REFERENCE_ROOT = \"Data/Arabic_Alphabets\"\n",
    "TARGET_LETTER = \"ÿßŸÑŸÅ\"\n",
    "MODEL_CACHE = f\"{TARGET_LETTER}_seq_cache.pkl\"  # store frame-level embeddings per ref file\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Controls how distance maps to similarity. You may tune this.\n",
    "# similarity = 100 * exp(-BETA * dtw_distance)\n",
    "BETA = 3.0\n",
    "\n",
    "# If your sequences are long, you can downsample frame dimension in time (take every k-th frame)\n",
    "FRAME_DOWNSAMPLE = 1  # set to 2 or 3 if you need speed vs accuracy\n",
    "\n",
    "# ---------------- Model load ----------------\n",
    "print(\"Loading model...\")\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(MODEL_NAME)\n",
    "model = Wav2Vec2Model.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "model.eval()\n",
    "print(\"Model loaded.\")\n",
    "\n",
    "# ---------------- Utilities ----------------\n",
    "def trim_silence_and_load(path, sr=16000, top_db=25):\n",
    "    y, _ = librosa.load(path, sr=sr)\n",
    "    yt, _ = librosa.effects.trim(y, top_db=top_db)\n",
    "    # if trimming removed everything, fallback to original\n",
    "    if yt.size == 0:\n",
    "        return y\n",
    "    return yt\n",
    "\n",
    "def extract_frame_embeddings(wav_path):\n",
    "    \"\"\"Return numpy array shape (T, D) of frame-level embeddings (no mean pooling).\"\"\"\n",
    "    y = trim_silence_and_load(wav_path, sr=16000, top_db=25)\n",
    "    inputs = feature_extractor(y, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "    input_values = inputs.input_values.to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_values)\n",
    "        frames = outputs.last_hidden_state[0].cpu().numpy()  # shape (T, D)\n",
    "    # optional downsample in time\n",
    "    if FRAME_DOWNSAMPLE > 1:\n",
    "        frames = frames[::FRAME_DOWNSAMPLE]\n",
    "    # normalize each frame (L2) to make cosine distances meaningful\n",
    "    norms = np.linalg.norm(frames, axis=1, keepdims=True)\n",
    "    norms[norms == 0] = 1.0\n",
    "    frames = frames / norms\n",
    "    return frames  # (T, D)\n",
    "\n",
    "def cosine_cost_matrix(a, b):\n",
    "    \"\"\"Return cost matrix where cost[i,j] = 1 - cosine(a[i], b[j])\"\"\"\n",
    "    # both a, b normalized per-frame -> cosine_similarity gives cosine\n",
    "    sim = np.dot(a, b.T)  # shape (T_a, T_b)\n",
    "    # clip numerical errors\n",
    "    sim = np.clip(sim, -1.0, 1.0)\n",
    "    cost = 1.0 - sim\n",
    "    return cost\n",
    "\n",
    "def dtw_distance_from_cost(cost):\n",
    "    \"\"\"Classic DTW dynamic programming on cost matrix. Returns final distance (float).\"\"\"\n",
    "    n, m = cost.shape\n",
    "    # dp matrix\n",
    "    dp = np.full((n + 1, m + 1), np.inf, dtype=float)\n",
    "    dp[0, 0] = 0.0\n",
    "    for i in range(1, n + 1):\n",
    "        for j in range(1, m + 1):\n",
    "            choices = (dp[i-1, j],   # insertion\n",
    "                       dp[i, j-1],   # deletion\n",
    "                       dp[i-1, j-1]) # match\n",
    "            dp[i, j] = cost[i-1, j-1] + min(choices)\n",
    "    return dp[n, m]\n",
    "\n",
    "def dtw_distance(seq_a, seq_b):\n",
    "    \"\"\"Compute DTW distance between two frame sequences (vectors normalized).\"\"\"\n",
    "    if seq_a.shape[0] == 0 or seq_b.shape[0] == 0:\n",
    "        return float(\"inf\")\n",
    "    cost = cosine_cost_matrix(seq_a, seq_b)\n",
    "    return dtw_distance_from_cost(cost)\n",
    "\n",
    "def distance_to_similarity(dist, beta=BETA):\n",
    "    \"\"\"Convert nonnegative distance to similarity percentage [0,100].\n",
    "       similarity = 100 * exp(-beta * dist)\n",
    "       Adjust 'beta' to calibrate sensitivity.\"\"\"\n",
    "    sim = 100.0 * math.exp(-beta * dist)\n",
    "    # clip\n",
    "    if sim < 0.0:\n",
    "        sim = 0.0\n",
    "    if sim > 100.0:\n",
    "        sim = 100.0\n",
    "    return sim\n",
    "\n",
    "# ---------------- Build cache (per-reference file frame sequences) ----------------\n",
    "def build_sequence_cache(reference_root, target_letter, cache_file=MODEL_CACHE):\n",
    "    folder = os.path.join(reference_root, target_letter)\n",
    "    if not os.path.isdir(folder):\n",
    "        raise FileNotFoundError(f\"Folder not found: {folder}\")\n",
    "    cache = {}\n",
    "    print(f\"Building sequence cache for letter '{target_letter}' from: {folder}\")\n",
    "    for fname in os.listdir(folder):\n",
    "        if not fname.lower().endswith(\".wav\"):\n",
    "            continue\n",
    "        path = os.path.join(folder, fname)\n",
    "        try:\n",
    "            seq = extract_frame_embeddings(path)  # (T, D)\n",
    "            cache[fname] = seq\n",
    "            print(f\"  processed {fname}: frames={seq.shape[0]}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  skipped {fname}: {e}\")\n",
    "    # save\n",
    "    with open(cache_file, \"wb\") as f:\n",
    "        pickle.dump(cache, f)\n",
    "    print(f\"Saved cache to {cache_file}. Total refs: {len(cache)}\")\n",
    "    return cache\n",
    "\n",
    "# ---------------- Compare a user file to target letter ----------------\n",
    "def verify_single_word(user_wav, cache_file=MODEL_CACHE):\n",
    "    # load cache\n",
    "    if not os.path.exists(cache_file):\n",
    "        raise FileNotFoundError(\"Cache not found. Run build_sequence_cache first.\")\n",
    "    with open(cache_file, \"rb\") as f:\n",
    "        cache = pickle.load(f)\n",
    "    user_seq = extract_frame_embeddings(user_wav)\n",
    "    if user_seq.shape[0] == 0:\n",
    "        print(\"Warning: user audio produced zero frames after trimming.\")\n",
    "        return 0.0\n",
    "\n",
    "    # compute DTW distance vs each reference, take minimum\n",
    "    best_dist = float(\"inf\")\n",
    "    for ref_name, ref_seq in cache.items():\n",
    "        try:\n",
    "            d = dtw_distance(ref_seq, user_seq)\n",
    "            if d < best_dist:\n",
    "                best_dist = d\n",
    "        except Exception as e:\n",
    "            print(f\"DTW failed for {ref_name}: {e}\")\n",
    "\n",
    "    similarity = distance_to_similarity(best_dist)\n",
    "    return similarity, best_dist\n",
    "\n",
    "# ---------------- CLI usage ----------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Build cache if not exists\n",
    "    if not os.path.exists(MODEL_CACHE):\n",
    "        build_sequence_cache(REFERENCE_ROOT, TARGET_LETTER, MODEL_CACHE)\n",
    "\n",
    "    # Example check\n",
    "    user_file = \"Data/user_input2.wav\"  # change as needed\n",
    "    sim, dist = verify_single_word(user_file, MODEL_CACHE)\n",
    "    print(f\"\\nDTW distance = {dist:.4f}\")\n",
    "    print(f\"Similarity to '{TARGET_LETTER}' = {sim:.2f}%\")\n",
    "    if sim >= 90:\n",
    "        print(\"Result: ‚úÖ Correct pronunciation.\")\n",
    "    elif sim >= 70:\n",
    "        print(\"Result: ‚ö†Ô∏è Partial / close.\")\n",
    "    else:\n",
    "        print(\"Result: ‚ùå Incorrect.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2896535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé§ Recording... Recite now\n",
      "‚úÖ Recording finished\n",
      "üß† Loading ASR model (Whisper Quran)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Transcribing...\n",
      "\n",
      "--- RESULTS ---\n",
      "üìù Expected   : ÿßŸÑŸíÿ≠ŸéŸÖŸíÿØŸè ŸÑŸêŸÑŸëŸéŸáŸê ÿ±Ÿéÿ®ŸëŸê ÿßŸÑŸíÿπŸéÿßŸÑŸéŸÖŸêŸäŸÜŸé\n",
      "üéß Recognized : ÿßŸÑŸÖ\n",
      "üìä Similarity : 15.0 %\n",
      "‚ùå Pronunciation: NEEDS IMPROVEMENT\n"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import re\n",
    "import Levenshtein\n",
    "from transformers import pipeline\n",
    "\n",
    "# =========================\n",
    "# SETTINGS\n",
    "# =========================\n",
    "DURATION = 5        # seconds to record\n",
    "SAMPLE_RATE = 16000\n",
    "AUDIO_FILE = \"recitation.wav\"\n",
    "\n",
    "EXPECTED_TEXT = \"ÿßŸÑŸíÿ≠ŸéŸÖŸíÿØŸè ŸÑŸêŸÑŸëŸéŸáŸê ÿ±Ÿéÿ®ŸëŸê ÿßŸÑŸíÿπŸéÿßŸÑŸéŸÖŸêŸäŸÜŸé\"\n",
    "SIMILARITY_THRESHOLD = 0.95   # 95%\n",
    "\n",
    "# =========================\n",
    "# RECORD AUDIO\n",
    "# =========================\n",
    "print(\"üé§ Recording... Recite now\")\n",
    "\n",
    "audio = sd.rec(\n",
    "    int(DURATION * SAMPLE_RATE),\n",
    "    samplerate=SAMPLE_RATE,\n",
    "    channels=1,\n",
    "    dtype=\"float32\"\n",
    ")\n",
    "sd.wait()\n",
    "\n",
    "print(\"‚úÖ Recording finished\")\n",
    "sf.write(AUDIO_FILE, audio, SAMPLE_RATE)\n",
    "\n",
    "# =========================\n",
    "# LOAD ASR MODEL\n",
    "# =========================\n",
    "print(\"üß† Loading ASR model (Whisper Quran)...\")\n",
    "\n",
    "asr = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=\"tarteel-ai/whisper-base-ar-quran\"\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# LOAD AUDIO MANUALLY (NO FFMPEG)\n",
    "# =========================\n",
    "audio_np, sr = librosa.load(AUDIO_FILE, sr=16000)\n",
    "\n",
    "# =========================\n",
    "# TRANSCRIBE\n",
    "# =========================\n",
    "print(\"üîç Transcribing...\")\n",
    "\n",
    "result = asr({\n",
    "    \"array\": audio_np,\n",
    "    \"sampling_rate\": sr\n",
    "})\n",
    "\n",
    "recognized = result[\"text\"]\n",
    "\n",
    "# =========================\n",
    "# NORMALIZE ARABIC TEXT\n",
    "# =========================\n",
    "def normalize(text):\n",
    "    #text = re.sub(r'[ŸëŸéŸãŸèŸåŸêŸçŸíŸÄ]', '', text)  # remove tashkeel\n",
    "    text = text.replace('ÿ£','ÿß').replace('ÿ•','ÿß').replace('ÿ¢','ÿß')\n",
    "    text = text.replace('Ÿâ','Ÿä').replace('ÿ§','Ÿà').replace('ÿ¶','Ÿä')\n",
    "    return text.strip()\n",
    "\n",
    "rec_norm = normalize(recognized)\n",
    "exp_norm = normalize(EXPECTED_TEXT)\n",
    "\n",
    "# =========================\n",
    "# COMPARE TEXT\n",
    "# =========================\n",
    "similarity = Levenshtein.ratio(rec_norm, exp_norm)\n",
    "\n",
    "# =========================\n",
    "# RESULTS\n",
    "# =========================\n",
    "print(\"\\n--- RESULTS ---\")\n",
    "print(\"üìù Expected   :\", EXPECTED_TEXT)\n",
    "print(\"üéß Recognized :\", recognized)\n",
    "print(\"üìä Similarity :\", round(similarity * 100, 2), \"%\")\n",
    "\n",
    "if similarity >= SIMILARITY_THRESHOLD:\n",
    "    print(\"‚úÖ Pronunciation: GOOD\")\n",
    "else:\n",
    "    print(\"‚ùå Pronunciation: NEEDS IMPROVEMENT\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725b0b75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quranic_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
